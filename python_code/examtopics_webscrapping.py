# -*- coding: utf-8 -*-
"""ExamTopics WebScrapping

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZlYbconI_NcfLk0QHklqZHqQA0SDI1Ce
"""

exam_name = "dp-600"
exam_links = f"/content/drive/MyDrive/ExamTopics/{exam_name}/{exam_name}_exam_topics_links_per_question_iterator.csv"

"""# Imports"""

from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.colab import drive
drive.mount('/content/drive', force_remount=False)
import pandas as pd
from google.colab import output
from googlesearch import search
from IPython.display import clear_output
import sys
import os
import base64
from PIL import Image
from io import BytesIO
#!pip install -U -q PyDrive

"""# Extract Vendor Name"""

query = f"examtopics.com {exam_name} Exam Actual Questions"
for j in search(query, tld="co.in", num=1, stop=1, pause=3):
  if j.startswith(f"https://www.examtopics.com/exams/") and exam_name in j:
            print(j)

vendor_name = j.split(sep="/")[4]
#vendor = "microsoft" # lowercase of Vendor Name, ie. sap, microsoft, oracle...
print(vendor_name)
TopLevelExamQuestionsURI = f"https://www.examtopics.com/exams/{vendor_name}/{exam_name}/view"

"""# Selenium setup"""

!pip install selenium
clear_output()

!apt-get update
!apt install -yq chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
clear_output()

from selenium import webdriver
from selenium.webdriver.common.by import By

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument("--window-size=1920,2160")

# Initialize the Chrome driver
driver = webdriver.Chrome(options=chrome_options)

try:
    # Open the URL
    driver.get(TopLevelExamQuestionsURI)

    #display(Image.open(BytesIO(base64.b64decode(wd.get_screenshot_as_base64()))))

    scrappedElement = driver.find_element(By.XPATH, "/html/body/div[3]/div/div[1]/div[2]/div/div/div[2]/div/ul/li[2]").text

    print(scrappedElement)

    total_number_of_questions = int(scrappedElement.split(sep=" ")[5])
    print(f"Number of questions: {total_number_of_questions}")

    # total_number_of_questions = 600
    # REST GET method to retrieve the number of total questions
    # https://www.examtopics.com/exams/microsoft/az-104/view/

finally:
    # Close the WebDriver
    driver.quit()

"""# Questions URLs extraction"""

links = []
for i in range(1, total_number_of_questions):
        query = f"examtopics.com/ {vendor_name} {exam_name} question {i} reveal solution"
        for j in search(query, tld="co.in", num=1, stop=1, pause=3):
          if j.startswith(f"https://www.examtopics.com/discussions/{vendor_name}/view/") and exam_name in j:
                if j not in links:
                    links.append(j)
                    print(j)
clear_output()

linksSeries = pd.Series(links)
linksSeries = linksSeries.drop_duplicates()
linksDataFrame = pd.DataFrame(linksSeries)

# links = []
# for i in range(1, total_number_of_questions):
#         query = f"examtopics.com/ {vendor_name} {exam_name} question {i} reveal solution"
#         try:
#           for j in search(query, tld="co.in", num=1, stop=1, pause=3):
#             if j.startswith(f"https://www.examtopics.com/discussions/{vendor_name}/view/") and exam_name in j:
#                   if j not in links:
#                       links.append(j)
#                       print(j)
#         except j.HTTPError:
#             os.kill(os.getpid(), 9)

# clear_output()

# linksSeries = pd.Series(links)
# linksSeries = linksSeries.drop_duplicates()
# linksDataFrame = pd.DataFrame(linksSeries)

linksDataFrame["topic#"] = linksDataFrame[0].str.split('-').str[5]
linksDataFrame["question#"] = linksDataFrame[0].str.split('-').str[7]
# linksDataFrame = linksDataFrame.sort_values(['topic#', 'question#'], ascending=[True, True])
#linksDataFrame.info()

topicsToAnalyze = linksDataFrame["topic#"].max()
questionsToAnalyze = linksDataFrame["question#"].max()

topicsToAnalyze = int(topicsToAnalyze) + 1
questionsToAnalyze = int(questionsToAnalyze) + 10

# link for link - link w liście wszystkich linków
""" Approach #1
fil_links = []
for link in links:
  if f"https://www.examtopics.com/discussions/" in link and f"-{exam_name}-topic-" in link:
    fil_links.append(link)
"""
# Approach #2
#filtered_links = [link for link in links if f"https://www.examtopics.com/discussions/" in link and f"-{exam_name}-topic-" in link]

#!pip install -U -q PyDrive
# exam_links = f"/content/drive/MyDrive/ExamTopics/{exam_name}_exam_topics_links_per_topic_per_question.csv"
#links = [] - use results from existing Series

for h in range (1, topicsToAnalyze + 1):
  for i in range(1, questionsToAnalyze + 1):
        query = f"examtopics.com/ {vendor_name} {exam_name} topic {h} question {i} reveal solution"
        for j in search(query, tld="co.in", num=1, stop=1, pause=3):
          if j.startswith(f"https://www.examtopics.com/discussions/{vendor_name}/view/") and exam_name in j:
            #if j.find(f"-question-{i}"):
                if j not in links:
                    links.append(j)
                    print(j) #monitor variables in Window, avoid mess on screen
clear_output()

linksQuestionsTopicsSeries = pd.Series(links)
linksQuestionsTopicsSeries = linksQuestionsTopicsSeries.drop_duplicates()
finalDataFrame = pd.DataFrame(linksQuestionsTopicsSeries)

finalDataFrame.melt() # unpivot series, to get the clear column with URLs

finalDataFrame.info() # if other name than "0" for the only column - then it is wrong

finalDataFrame["topic#"] = finalDataFrame[0].str.split('-').str[5].astype(int)
finalDataFrame["question#"] = finalDataFrame[0].str.split('-').str[7].astype(int)
finalDataFrame = finalDataFrame.rename(columns={0:"URL"})
finalDataFrame = finalDataFrame.sort_values(['topic#', 'question#'], ascending=[True, True])
#finalDataFrame.head()

print(finalDataFrame.to_markdown())
finalDataFrame.to_csv(exam_links, index=False)

extracted_questions_number = int( len(finalDataFrame.index) )
accuracy = f"{extracted_questions_number/total_number_of_questions:%}"
print(accuracy)

for i in range(1, total_number_of_questions):
        query = f"examtopics.com/ {vendor_name} {exam_name} question {i} reveal solution"
        try:
          for j in search(query, tld="co.in", num=1, stop=1, pause=3):
            if j.startswith(f"https://www.examtopics.com/discussions/{vendor_name}/view/") and exam_name in j:
                  if j not in links:
                      links.append(j)
                      print(j)
        except j.HTTPError:
            os.kill(os.getpid(), 9)

import os
import time

breakPointValue = 43

arr = [10, 5, 0, -5, -10]

for i in range(1,len(arr)+1):

#for i in range(-50):

  try:
      print(arr[-i])
      print(i)
      print(i/0)
      # if i >= breakPointValue:
      #   os.kill(os.getpid(), 9)
  except: #i == breakPointValue:
        #os.kill(os.getpid(), 9)
        print("got move to 'except', restart in 30 seconds")
        time.sleep(30) # sleep for 30 seconds
        exit()
        #os.kill(os.getpid(), 9)
        print("awaking after restart...")
        continue
        #exit()

  finally:
      continue

